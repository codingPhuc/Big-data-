





Data Analytics Software Stack  

we used hdfs to store data  
we used spark to store  


# PySpark  
framework chung trong do  se co framework cho python  , written in python programming lanauge , in this course we will play atention to spyspark only to , used pyspark you should master programming l


Spark application is used 
there are two way to execute pyspark and input instruction one by one ,  python you can open the terminal and execute the sh shell in this shell you can type into it instruction  
and they are executed one by  one pyspark also support a cell like that we just for professional job you have to write source code to file and submit the file to  file system 



# Spark context  

trong hệ thống  k để chức năng rời rạc thường tạo ra với lớp đối tương , mình tương tác với hệ thống qua lớp đối tượng đó , Spark context là một lớp đối tương qua cái Spark context này mình có thể tương tác với các method 
chức năng 
đọc ghi file system 
sử lý trên file system 
tương tác với cluster (cloud)
một trương trình spark context chạy dc trên 2 hệ thong local  file system và cloud 

parameter 
master - duong dan den cluster ( cluster co duong dan gi thi bo vao )
appName ten cai job (dat cho de hieu) 
sparkHome  duong dan den thu muc cai dat spark 
pyFIles 
serializer chuyen doi object trong bo nho thanh byte array  nguoi nhan co the decode cai file array do de co lai cai object ban dau 
conf  co nhieu cai key value pair de cai key configuration  dong goi tao ra mot cai spark config 
Jsc JavaSpark context 
profiler_cls 



khi chay mot truong trinh thi se co  3 thiet bi stdin ,  stdout ,  stderr  (thiet bi xuat chung , thiet bi nhap trung , thiet bi loi trung )



RDD  reselient distributed dataset  ,   mot lich su mua hang thi nhieu gio hang 1 tap nguoi dung thi nhieu vector thong tin ,  RDD la mot object giup chung ta luu tru cai du lieu  do  
, co hai loai RDD  tranformation la tao tap du lieu moi , action la tao ket qua 
tranformation 
- filter 
- phan tu dat dieu kien 
- map 


collect là lênh nguy hiêm tao ra môt cai list du lieu trong python , kha nang crash may làm cho may het hoat dong 



# Sending data in a network 

inicast one sender to one receiver 
broadcast send data to all receiver 
multicast send data to group of receiver 

broadcast variable is only cache that store data in memory only  , broadcast variable are
there are many type of parameter , 
sc mean spark context  , we create a variable sc mean spark context 
went we want to initiated broad cast we used the constructor 


## how to tell if something is comutative 
add(a,b) =   add(b,a) 
## how to tell if something is associative 
f(f[a,b] ,c)  =  f(a, f[b,c]) 
add(add[a,b],c)  =  add(a,add[b,c]) 



example 
is max  comutative or associative 
max (a,b)  = max (b,a) 
->  max is a comutative operation 
max([a,b] ,  c)   = max ([b,c],a) # associative 
min  


in the operation system when multiple process want to access another resource it will have multiple excusion error 


spark configuration 

for spark application 



spark sử dụng hai thư viện python để tao serializer cho nó đó là mashalSerializer và PickleSeriaclizer 
dung mac dinh Pickleserializer neu 
neu muon lam do an thi phai tra tren documentation cua spyspark  
