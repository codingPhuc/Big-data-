# Introduction to Hadoop  

- Define Hadoop 
- Explain the history of Hadoop 
- List reasones why Hadoop was the answer to Big Data processing 
- Outline some of the challenges of Hadoop 
## what is hadoop 
hadoop is an open-source framework used to process enormous data sets 
Why Hadoop ? 
Hadoop was designed to  help organization manage terabytes of data
used to process unstructured data with relational Data 
hadoop is a set of open source programs and procedure 
used  for processing large amount of data 
server run application on [[cluster architecture]]
Handle parallel jobs or processes 
## Type of big Data 
Structure Data 
Semi-Strcuture Data 
unStructure data 

history 
apache software fondation estblised 
nutch web search engine created 
nutch was divided into web crawler , distributed processing 

# How does Hadoop work 
Hadoop Common is an essential part of Apache Hadoop framework that refers to the collection of common utilities and libraries that support other Hadoop modules 

HDFS 
- Hadoop Distributed File System  
- Handles and stores large data 
- Scales a single Hadoop Cluster into as much as thounsand cluster 
MapReduce  
- known as Hadoop processing unit 
- Process  Big Data by splitting the data into smaller units
- The first method used to query data stored in HDFS 
Yarn 
- Yet Another resource Negotiator acronym 
- Prepares , Hadoop for batch , stream , interactive and graph processing 

## The challenges of Hadoop  
processing transaction (random access)
when work cannot be parallelized 
when there are dependencies in the data 
low latency data access 
- low latency allow small delay  unnoticeable  by human by input being process and  corresponding output providing real time characteristics 
Processing lot of small files 
Intensive calculation with little data 




## introduction to mapReduce 

- Eplain the terms Map and Reduce in MapReduce 
- Describe why we use  MapReduce 
- List The componets of MapReduce 
- Outline examples of common use cases of MapReduce 

## what is MapReduce 
Programming model used in Hadoop for processing Big data 
Processing technique for distributed computing bases on java
- Distributed computing is a system with multiple components located on different machines that communicate actions in one view to the end user 
consist of a Map task and a Rudce task 
can be coded in may programming language C++ , java  , python 
## Map and Reduce 
Input file 
map: processes map  into key value pairs 
further data sorting an organizing 
Reducer Aggregates and computes a set of result and produces a final ouput 
MapReduces keeps track of its task by creating a unique key to ensure that all process are solving the same problem 

the input in the map section is a file that is save in HDFS 

an example :
![[mapreduce2.PNG]]
input file that have name of people we want to do a word count 
- each line in the file is split into each section 
- the map will then split combine it into key value 
- the reducer  start with suffering , you will see the key  Teju combine to [1,1,1] indicating how may time it occurred in the file it then output it in the output file  


WHy use mapreduce 
ability to allow for [[parallel processing]] accross multiple node , a node is an independent computer for processing and storing big volumnes of data 
Hadoop we have two types of node the name node and the data node 
Map reduce allow for spliting and running independent task in [[parallel processing]] by dividing each task saving time 
mapreduce can process data that come in tabular and non-tabular form 
MapReduce provides buiness value to origanization does't matter how buiness is strucutre 

common used cases 
used for social media to see who view your profile 
user for bank to flag user trasaction 
advertism to under stand user behavior 


# Hadoop Ecosystem 

List the stages of theHadoop Ecosystem 
Differentiata between the core componets and the extended componets 
List some example of tools used in each stage 


Hadoop common is the common util that support toher hadoop model 

HDFS stores the data coolletd form the ingestion and distributed data across multiple node 
mapreduce process them in clustor 
yarn is clustor management system 



The hadoop ecossytem is made up of componets that support one another 

![[hadoop Ecosystem framwork.PNG]]
- Flume and Sqoop  are responsible for ichesting the data and tranfer them the hdfs and HBase
- the go to Pib and Hive for analyazing the data 

### Ingesting 
Ingesting is the first stage of Big Data processing  whever you dea; with big data you get data from different sources  
Flume  
- Clooect , aggreage and transfer big data 
- haas simgpel and flexible architecture bases on streaming data flows 
- Used a single extensible data model allows for onlien analytic application 
Sqoop 
- Designed to tranfer data between relational database system and Hadoop 
- Accesses the database to understand the schema of the data 
- Generates a MapReduce application to import into HDFs 
### Store data 
Hbase 
- a non relational database that run on top of HDFS 
- Provide real time Wrangling on data 
- Stores data as indexes to allow for random and faster access to data 
Cassandra 
- A scalable , NoSQL database designed to have no single point of failure 
Analyze data 
Pig 
- Analyzes large amount of data 
- Operates on the client side of a cluster 
- a procedural data flow language 
Hive 
- Used for creating report 
- Operates on the server side of a cluster 
- A declarative programming language 

Access data 
Impala 
- Scalable and easy to used plat form for everyone 
- no programming skill require 
Hue 
- Stand for Hadoop user experience 
- Allow you to upload , browse and query data 
- Run Pig jobs and workflow 
- Provides editor for server SQL query languages like Hive and MySQL

HDFS 
explain Hadoop Distributed File System(HDFS) and important concepts 
Identify the key HDFS features 
Describe the HDFS architecture 

## HDFS 
HDFS is the acronym for Hadoop distributed File System 

DFS is a file ssytem that dis on multiple file server and allow programer to access and store file any where on the system 
HDFS is the storage layer ofHadoop 
Split the files into blocks, creaes replicas of the blocks , and stores them on differenet machiens 
Provides access to streaming data 
Streaming mean that HDFS provides a constant bitrate when transferring data rather than   having the data being tranfer by waves 
HDFS used GUI interact 

## key features 
Cost efficient  
- the storage hardware is not expensive 
Large amounts of data 
- HDFS can store up to petabytes of data 
Replication 
- make copies of the data on multiple machines 
Fault tolerant 
- if one machines crashes , a copy of the data can be found somewhere else and work continues 
Scalable 
- One cluster can be scaled into hundreds of nodes 
Portable 
- can easily move across multiple platforms 

## HDFS concepts 
when HDFS receive file the file are broken into block 
block is the default block size that the file can be read in 
- minimum amouint of data that can be read or written 
- Porvides flault tolerance 
- Default size is 64MB or 128MB 

Block 
![[blocksize.PNG]]
- if we have a 500 MB with the default size of 128mp 
- the file will have 4 chunk with 128mb 
- each file stored doesn't have to take up the configured space size 
Nodes 
A node is a single system which is responsible to stores and process data 
there are two type of node in HDFS Primary node and Secondary Nodes 
![[node in HDFS.PNG]]
- Primary node aka NameNode 
- Secondary node DataNode 
Rank awerness in HDFS 
- Choosing data node reack that are closest to each othere 
	- A rack is the collection of about 40 to 50 data nodes using the same network switch 
 - Improves cluster performance  by reducing network traffic 
 - Name node keep the rack ID information 
 - Replication can be done thought  rack awareness 
Replication 
- Creating a copy of the data block 
- copies are created for backup purposese 
- Replication factor : number of times the data block was copied 
![[repittion.PNG]]
- the number of replication can be bases on a user choice , an example is the file that was split into different chunk each chunk is replicated 2 time and store in different rack so then if one rack fail the other rack will store the file copy 
HDFS concepts
Read and write Operation 
- HDFS allows write one read many operation 

primary secondary node architecture 
![[file node architecture.PNG]]
the primgaru node is the name node per custor there is one name node and many second dary node , the file is split into different fblock  which is store on the data node 
name node oversse 
opping closing reanming file operations and mapping file blocks to  data node 
data nodes 
read and write request from the client and perform creation replication and deletion of file block bases on the name node 

# Hive 
articulate the resound why HIve is used List Hive features 
differentiate between HIve vs traditional  RDBMS 
Identify componet of the Hive architecture 
Discuss Hive concept 

Hive is data warehouse  software within Hadoop that is designed for reading , writing and managing tabular-type dataset and data  analysis 
- it is scalable , fast and easy to use 
- Hive Query Language (HiveQL ) is inspired by SQL , making it easier for users to grasp cconcepts 
- it support data cleaning and filtering depending on users requirement 

RDBMS - is design for relation database 
A relation databse store data in structure format in row and column 
traditional RDBMS used to maitian database


| Tradition la RDBMS | Hive |
| ---- | ---- |
| Used to maintain a Database  and uses SQL | Used to maintain a data warehouse using Hive query language |
| Suited for real-tome/dynamic data analysis like data from sensors  | Suited for static data analysis like text file containing names  |
| Designed to read and write as many times as it needs  | Designed on the methodology of write once , read many  |
| Maximum data size it can handle it terabytes  | Maximum data size it can handle is petabytes  |
| enforces that the schema must  verify loading data before it can be proceed  | Doesn't enforce the scheam to verify loading data  |
| May not alway have built in for support data partitioning  |  support partitioning  |

![[HIVE.PNG]]
three main part of the architecture : 
- Hive  provides different drivers for communication depending on the type of application ( java bases used JDBS other used OCDB) communicating with the server 
- any query operation use HIVe serveces , the CLI act as an interface driver take in query , monitor secsion and store the metadata , the metastore file system job clien perform the following 
	- Metadata information of tables are stored in some sort of database and query results 
	- data loaded in the tables are stored in Hadoop cluser or HDFS 

JDBC 
JDVC client allow Java application to connect to hive 
OCDB 
allow client using OCDV architecture to connect to hive 
HIve serviecs section is in charge of running query 
Hive Server to enable queries 
- The driver receives query statement 
- the optimzer is used to qplit task efficently 
- The executor executes the executes task after the optimizer 
- Metastore stores the metadata information about the tables (Metastore keep these data in a centure place)



# HBase 
- Define HBase 
- Describe HBase as columnar database 
- List the HBase feature and usage 
- Outline the difference between HBase and HDFS 
- Describe the HBase architecture 

HBase is a Column-oriented non-relational database management system 
HBase runs on top of HDFS 
Provides a fault tolerant way of storing sparse dataset  
- Fault tolerance refers to the working ability of a system or computer to continue working even in unfavorable condition such as when a server crashes 
works well with real time data and random read and write access to Big Data  

HBase is used for write heavy applications 
- HBase is linearly and modularly scalable 
- it is a backup support for MapReduce job 
- It provieds consistenread and writes 
- It has no fixed column schema 
- it is an easy-to-use Java API for client access
- It provides dta replication across cluster 

![[HBase.PNG]]
- HBase work in the way that there is a familles connection between the column where multiple column can be group inside into a families columns the family columns will be a add in idicator telling which group the individual columns belong to 

| HBase                                                               | HDFS                                                                           |
| ------------------------------------------------------------------- | ------------------------------------------------------------------------------ |
| HBase stored dta in the form of columsn and rows in a table         | HDFS stores data in a distribured manner across diffrerent node on the network |
| HBase allow dynamic changes                                         | HDFS has rigid architecture that deos not allow changes                        |
| HBase allow for storing and processing of Big Data                                                                    | HDFS is for storing only                                                                                |
| HBase is suitable for random writes and read of data stored in HDFS | HDFS is suited for write one and read many times                               |
![[arhichtecture HBase.PNG]]
Four component 
HMaster 
- Monitor the Region  server instances 
- assigns region to region servers 
- Manages any changes that are made to the schema 
HRegion server 
- Receives and assigns request to regions 
- Reponsible for managin regions
- Communicates directly with the client 
HRegion  
- Smallest unit of HBase cluster 
- Contain multiple stores 
- Two component -HFile and Memstore 
ZooKeeper 
- Maintains healthy link between nodes 
- Provides distributed synchronizastion 
- Track server failure 
HBase work top of HDFS 




- Hadoop is an open-source framework for Big Data that faces challenges when encountering dependencies and low-level latency.
    
- MapReduce, a parallel computing framework used in parallel computing, is flexible for all data types, addresses parallel processing needs for multiple industries, and contains two major tasks, “map” and “reduce.”
    
- The four main stages of the Hadoop Ecosystem are ingest, store, process and analyze, and access.
    
- Key HDFS benefits include its cost efficiency, scalability, data storage expansion, and data replication capabilities. Rack awareness helps reduce the network traffic and improve cluster performance. HDFS enables “write once, read many” operations.
    
- Suited for static data analysis and built to handle petabytes of data, Hive is a data warehouse software for reading, writing, and managing datasets.
    
- Hive is based on the “write once, read many” methodology, doesn’t enforce the schema to verify loading data, and has built-in partitioning support.
    
- Linearly scalable and highly efficient, HBase is a column-oriented nonrelational database management system that runs on HDFS and provides an easy-to-use Java API for client access.
    
- The HBase architecture consists of HMaster, Region servers, Region, Zookeeper, and HDFS. A key difference between HDFS and HBase is that HBase allows dynamic changes compared to the rigid architecture of HDFS.