# single node architecture 
- the algorithm run in the cpu then access the memory , the data does not need to be loaded again 
- what happend when the data is loaded in again 
	- classical data minining algorithm look at the disk you get the data on the disk then process the disk data 
	- example google example : 
		- 10 billion web pages average size of web page =20kb  -> 200 TB 
		- data band width between the data and the cpu disk read bandwidth = 50mb/sec 
		- time to read 4 million second = 46 + day 
		- even longer to do something useful with the data 
		- you can read it into multiple disk and cpu which will cut down the data so like 4 milion cpu and read it 4000 s which is about an hour 
## cluster architecture 

![[CustorArchitecture.PNG]]
- you have rack containing linux nodes   
- each rack 16-64 linux nodes these node are connected by a switch  
![[multiplerack.PNG]]
- on rack is not enough you need at least more rack conneneted with a switch 
## challenge of cusltor computing 
- 2011 google have 1m machine running like this 
- Node failutes 
	- a single server can stay up for 3 years (1000 days) 
	- 1000 server in cluster => 1 failure /day  
	- 1m server in a cluster => 1000 failures /day 
how to store data persistently and keep it available if node can fail 
how to deal with node failures during a long running computation 
- infrastructure to hide these node failure and comple the computation even if node fail 
netword bottleneck 
- network bandwidth = 1gbps 
- moving 10tb take approximately 1 day 
distributed programming is hard 
- hard to write program correctly to avoid race condition 
- you need to simple model that hides the most of the complexity 
**map reduce** address the challenges of clustor computing 
- store data redundantly on multiple nodes for persistence and availability 
- move computation close to data to mionimize data movement 
- simple programming model to hide the complexity of all this magic 
## redundant storage infrastructure 
distributed file system 
- provide global file namespace , redundancy and availability 
-  google GFS , Hadoop HDFS 
Typical usage pattern 
- huge file (100s of gb to Tb)]
- data is rarely updated (when updated it only updated thouhg a pen not in place ) 
	- when google encounter a new webpage add a webpage to a repository not updated the webpage it already have 
distributed file system 
- these file are keep into chunk and these chunk are spread  across machines 
![[chunk in file system.PNG]]
- data are keep in chunk that are divided into multiple machine 
- as we can see 
	- chunk server 1 have 4 chunk 
	- server 2 have 1 chunk 
	- chunk server 3 have 1 chunk 
![[chunkserver2.PNG]]
- we can see that chunk server are duplicated in different chunk server 
- chunk server as compute servers when computation is done on the data the computation is move to the chunk server that way we do not need to move the data just the computation 
## chunk server 
the switch on a rack fail making it so the whole chunk is unreplicatable 
replica of chunk is on different so that the data is not missing 
## master node 
store the master data where the file is store 
- like file 1 is store on 6 chunk 
- the master node is importance 
## client libary 
- the clent libary talk to master find chunk that store the chunk then access the data without going thougth the master node 


# MapReduce 
## warm up task 
imagine you have a huge document you want to count each time a word apper in a file like apple or bird 
- some thing like a search engine 
let just imaging that we have big document and count where the file apper 


case 1 : 
- file to large for memeory , but all <word , count > pairs fit in memory 
	- hashtable word -> count 
		- the first time you see the word increment the count by one then add the count that increment the count for each time the word appear 
case 2 : 
- even the <word ,count> pair don;t fit in memory 
	- word (doc,txt) | sort | uniq -c 
		-  where `word` takes a file and output the words in it , one per a line this is a Linux command line argument  
		- the command word is a little script that output the word in dox.txt 
		- then sort the word output 
		- all occerenced of the same word com togheter 
		- uniq -c does is count the current of same word 
		- the output is a word pair 
- case 2 cpatures the essence of mapreduce 
	- word (doc.txt)
		- Map 
			- scane input file record at a time 
			- extract somethong you care about form each record (keys)
		- Group by key 
			- sort and shffle 
		- Reduce 
			- aggregate , summarie , filter or transform 
			- write the result 
- out line stay the same , map and reduce the problem 
![[mapreduce.PNG]]
- the map function take in input form as a `<key,value>` pair ,processes them , and produces another set of intermediate <key,value> pairs  
	- the mapper is used for parallel computation each mapper will be assign a chunk of the input data to read and output intermediate key for them  
- these intermediate key value pair are then process by the combiner  .   The combiner is a reducer that run individually on each mapper server it reduces the data on each mapper further to a simplified form 
	- the combiner sort or combine  pairs that have the same key , to make it easier for the reducer to reduce 
  - after all the mapper  have done processing and the combinator done shuffles and sort the result , all map output will be funnels  into a single reducer, which then aggregates the values 
	  - the reducer will format the result this could be counting the number of time each key appears and grouping them into a single output like the picture above  
### more formally 
**input** : a set of key value pair 
map method take a input key value pair and output a set of intermediate  key value pair 
there is one map for each key value pair 

**reduce** 
take in a intermediate key value group intermidate value group is a key  the output is one or more key value pair the output is the same as the input key and the output is different in some manager 




- we will have word document  
- the map function read the input and output a set of key value pair 
- the group by key function sort and collect the value together 
- the reduce function collect all values belonging to the key output 
- the data is divided by multiple node by the illustrated by the red line in different node 
- when the map function is output and read spread across multiple node , what the system does is copy the map value form multiple node then combine it in the combine function 
- you can also tell the system to used 3 reduce node , the system is also smart it will sort the word that are with the same key end up with the same reduce node ( it used hashmap for this )
- the result will then spread out in 3 node 
- all the map reduce magic is using only sequential read of disk not random access of disk  , it take much longer to do random access compare to sequential read 
```
map(key , value) 
// key : document name ; value ; text of the document 
for each word w in value : 
	emit(w,1) 

reduce(key, value): 
	result =0  
	for each count v in values : 
		result += v 
	emit(key,result)
```
## host size 
suppose we have a large web corpus with metadata file formated as follows :
each record of the form (URL  , size ,data, ... )
for each host , find the total number of bytes 
note the url alone cannot determine the host because there will be host with multiple url 
****
**Map** 
for each record , ouput (hostname(URL) , size ) 
**Reduce** 
sum the sizes for each host 

