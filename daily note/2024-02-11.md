Why use Apache Spark  
Describe Apache Spark attributes 
Describe distributed computing 
List the benefits of Apache Spark and distributed computing 
compare and constast Apache Spark to MapReduce 


## what is Distributed computing 
A group or cluster of computer working together to appear as one system to the end user 
The term distributed computing often used interchangeably with parallel computing as both are similar 
![[distributed vs parrelel.PNG]]
parallel computing processors access shared memory 
distributed computing proccessor usually have their own private or distributed memory 
## Distributed computing benefits 
Scalability and modular growth they work with multiple machine and work horizontally 
fault tolerance and redundancy  they used node that can fail , redundancy that enable business continuity  for example business running 8 machine can function if one or multiple machine stay off line 

# Spark   

Spark is an **open source** in-memory application framework for distributed data processing and iterative analysis on massive data volumes 
in memory 
all operation happend in the memory or ram 
distributed data proccessing used spark 


Spark is written in Scala and runs on Java virtual machine  

Spark check the boxes for all the benefits of distributed computing 
support a computing framework for large scale data processing and analysis 
provide parallel and distributed processing scalability and fault tolerance on commodity hardware 

## Benefits 
provide speed due to in-memory processing 
create a comprehensive unified framework to manage big data processing 
enable programming flexibility with easy to used Python Scala and Java API 


## Apche SPark & Map Reduce Compared 
Traditional Approach : 
Crate MapReduce jobs for complex jobs , interactive query and online event hub processing  involves lots of slow disk I/O 

![[spark.PNG]]
- interaction that require read and write to disk , read and write are usullay time consuming and expensive 

Solution 
- keep more data inmemory with a new distributed execution engine 
![[spark mapreaduce.PNG]] 

## data engineering 
- Core Spark engine 
- Clusters and executors 
- Cluster management 
- SparkSQL 
- Catalyst Tungsten [[DataFrame]] 
Data science and Machine learning 
- SparkML 
- DataFrames 
- Streaming 


# Functional programming basic 
- Explain the term "functional programming"
- explain lambda function 
- A mathematical "function" programming style 
- Follows a declaration programming model 
- declaration emphasized "What" instead of "How-to" 
- Used "expression" instead of "statements "
# What is functional programming 
- first implementation was LISt Programming Language (LISP) in the 1950  
- Scala most recent representative (Python , R and Java also provide rudimentary support for functional programming) 
- functions are first class citizen in Scala 


![[functional.PNG]]
- we first create a a function f(x) = x+1  
- apply the function with a list of number 
- then the function will increment the list by one 

this is the traditional preceduaral way to achive the same result requiering
```
def incremnt(mylist) : 
N = size(mylist) 
	for i in range(N) : 
		myList[i] +=1  
	return myList
```
- this code oberate on the how instead of on the what solution


paraleleization is one of the main benefit of functional programming   
![[function parrlecomputing.PNG]]
you do not need to change the function definition or the code you just need to run one instance of the task more than one time you are able to scale the program running on multiple node 

Lambda 
Lambda calculust  calculus function or operator , are anonymous function that enable functional programming 


Spark parallelizes computation using the lambda calculus 
all functional spark program are inherently parallelizatble 



## parallel programming using distributed data set refer too as RDDs 

- Define Resilient Distributed Datasets(RDDs)
- Define Parallel Programming 
- Expalin Resilience in Apache Spark 
- Relate RDD and parallel Programming with Apache Spark 


## what are RDDs 
A resilient distributed dataset : 
Spark primary data abstarction 
a fault tolerant collection of elemnt 
partitioned across the nodes of the cluster 
capable of accpeting parallel operation 
immutable  (these database cannot be change one created )

Spark conssiste of a [[driver]] program that run the user main function and multiple parallel operation on a cluster  

RDD support files 

| Support file types | support file format |
| ---- | ---- |
| text | Local |
| sequence files | Cassandra |
| Avro | HBase |
| Parquet | HDFS |
| Hadoop input format | Amazon S3 |
|  | SQL and NoSQL |
|  |  |

Creating an RDD in spark 
used an external or local file form  

## Creating and RDD in Spark 
![[rdd.PNG]]

Use and external or local file form Hadoop supported file ssytem such as 
- HDFS 
- Cassandra 
- Hbase 
  - Amazon S3 

Simple examples of creating a RDD from a list in Scala and Python 

![[spark rdd.PNG]]]
this simple code using both scala create an RDD Spark run one task for each partition of the cluster you want two to four partition for each cpu in your cluster , you can set partition mannually by setting the partition number on the paralized function 

![[RDD in spark.PNG]]


Apply a transformation on an existing RDD to create a new RDD 

Parallel Programming 
- Parallel programming : 
- is the simulaneous use ofr multiple compute resources to solve a computational problem 
- reak problem into dicreate part that can be solved concurrently 
- run simultaneous instruction on multiple processors 
- employs an overall controll 

you can create an RDD by parallelizing an array of Object or by splitting a dataset into partitions 
Spark run one task for each  partition of the cluster 

RDD (Resilient Distributed Dataset) 
are always recoverable as they are immutable 
can persist or cache dataset in  memory across operations which speeds interative operation 
when you persiste an RDD each node store   the partions that the node computed in memory and reused the same partition in other actions on that data set or the subsequence data derive on the first RDD 
persistence allow futer action to be much faster by 10 time 
persisting or caching is used as a key tool for interative algorithm and fast interactive used 



# Scale out / Data Parallelism in  Apache Spark  
there are three different componet in spark 
![[spark component.PNG]]
consists of three main components 
Data storage 
- data set load from data storage into memeory 
- any hadoop compatible data source is acceptable 
Compute interface 

cluster managment 
- handle the distributed  componet aspect of spark 
- spark cluster managment can exit as it standard custor managment framwork 


data from the hadoop file system flow into the compter interface or API which then flow into different for distributed and parrallel task 

## Spark Core 

what  used to be call spark is the bases engine call spark core 
is the base engine 
is fault tolerant 
perform large scale parallel and distributed data processing 
manages memeory 
schedules task 
house APIs that define RDD
Spark core aslso parallelizes a distrubted collection of element across the cluster 



Scaling big data in Spark 
![[spark working.PNG]]

The Spark aplication is comprize of [[driver]] program and executor program executer program run on worker nodes Spark can start additional processes on worker if there is 
excuter can take multiple core for multi thearded operation 
 Spark distributed RDD amount executer communication happend amount the driver and the executor  The driver contain the spark job that the application can run the driver 
 receives the task result when the executor complete the task 
 example : 
 If apache spark were a large organization or company   the driver code would be the executive management of that company that makes decision about allocating work obtaining capital and more  the the junior employees are the executor who do the jobs assigned to them with the resources provided , The worker nodes correspond to the physical office space that the employees occupy 
 ![[worker scalling.PNG]]




# Spark SQL and Data Frame 



Is a Spark module for structured data processing 
Used to query strutured data inside Spark program , using either SQL or a familiar DataFrame API 
Usable in Java , Scala , Python and R 
Run SQL queries over imported data and existing RDD independently of API or programming language 
## Spark SQL Benefits 
includes a cost based optimizer ,columnar storage , and code generation to make queries fast 
Scale to thousand of node and multi hour queries using the spark engine which provides full mid query fault tolerance provides a programming abstraction called dataframes and can also act as a distributed SQL query engine 
## dataFrames 
Distriubted collection of data organized into name columns 
conceptually equvalent to a table in realtional database  or a data frame in R/Pythn , but with richer optimizations 
built on top of the RDD API 
uses RDDS 


Python code snippet to read from a JSON file and create a simple DataFrame 


## Data Frame Benefits 
Ability to scale from kilobyte of data on a single laptop to petabytes on a large cluster 
support for a wide array of data format and storage system 
state of the art optimization and code generation 
seamless integration with big data tooling and infrastrucutre via Spark 
API for Python Java Scala and R which is in development via Spark R