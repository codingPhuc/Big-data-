

Apache spark user interface 
- connect apche  user interface  server 
- navigate user inter 
websever user in 
`https:// driver node`
context stop application UI is no longer usable 
jobs tab  show 
storage of presisted RDD and dataframe 
environemnt configuration and properties 
executor summary  
SQL information (if sql queires exit)



Spark job interface 

![[Pasted image 20240526103124.png]]
Sure, based on the image you’ve shared, here are the highlighted tabs in each section of the Apache Spark User Interface:

1. **Details for Job 1 Section:**
    
    - **Completed Stages:** This section includes a table with columns like ‘Stage ID,’ ‘Description,’ ‘Submitted,’ ‘Duration,’ and ‘Tasks: Succeeded/Total.’ It provides information about the different stages of a specific job.
    - **DAG Visualization:** This is a graphical representation of the stages and tasks of the Spark job. It helps you understand the flow of data and computation in your application.
2. **Instructions Section:**
    
    - **View the Job Stages timeline and job status:** This instruction guides users to view the timeline of job stages and their statuses.
    - **Quickly view stage details based on status:** This instruction suggests users to view the details of stages based on their current status (completed, active, or pending).
    - **Click the Description link to view completed stage details:** This instruction directs users to click on the ‘Description’ link to view more details about a specific completed stage.


![[Pasted image 20240526103510.png]]

- **Stages**: These are the various stages of computation for all jobs. Each row represents a different stage with its own set of tasks.
- **Description**: This column provides a brief description of each stage. Clicking on the description links will provide more detailed information about each computation stage.
- **Submitted**: This column shows when each stage was submitted for execution.
- **Duration**: This column displays how long each stage took to complete.
- **Tasks: Succeeded/Total**: This column shows the number of tasks that succeeded out of the total number of tasks for each stage.
- **Input: Bytes/Records**: This column shows the amount of data processed in each stage, both in terms of bytes and the number of records.

![[Pasted image 20240526103527.png]]
- **ID**: This column lists the identification number of each Resilient Distributed Dataset (RDD).
- **RDD Name**: This column provides the name of each RDD. Clicking on the RDD name will provide more detailed information about that particular RDD.
- **Storage Level**: This column indicates the storage level of each RDD, which can be MEMORY ONLY, DISK ONLY, or a combination of both.
- **Cached Partitions**: This column shows the number of partitions of the RDD that are cached.
- **Fraction Cached**: This column shows the fraction of the RDD that is cached.
- **Size in Memory**: This column displays the size of the RDD that is stored in memory.
- **Size on Disk**: This column shows the size of the RDD that is stored on disk.

![[Pasted image 20240526103541.png]]
![[Pasted image 20240526103554.png]]

![[Pasted image 20240526103609.png]]







- **Executor Memory** is used for computation in Spark tasks.
- **Storage Memory** is used for caching partitions of your data that you want to persist for faster access during computation.

The key point is that these two types of memory share a unified region within the Java Heap Space. Executor memory can evict storage memory if necessary, but only until total storage memory usage falls under a certain threshold. This threshold is represented by a reserved area where cached blocks are immune to eviction. However, storage memory is not allowed to evict executor memory.
![[Pasted image 20240526122126.png]]


# Benefit of spark data persistence 
- Store intermediate calculations 
- persist to memory/ disk  
- less computation 
- Faster iterations over data 


![[Pasted image 20240526122327.png]]
When you call the `.cache()` method on a DataFrame, Spark keeps the data in memory after it’s computed during an action. This means that if another action is called on the DataFrame, Spark doesn’t need to recompute the data from the source, but instead, it fetches the data directly from the cache. This can significantly speed up your computations.

In your code snippet, you’re creating a DataFrame with a “features” column filled with random values. You then cache this DataFrame. The first time an action (like `count()`) is applied, the “features” are generated and cached. When you call another action, the cached DataFrame is reused, and Spark doesn’t need to recompute the “features” column. This is a good demonstration of the benefits of caching in Spark.


# Understanding processor resources 
![[Pasted image 20240526123105.png]]

 starting a second application with two cores means initiating a new Spark job with its own driver program and Spark context, and this new job will run tasks on two cores of the executor. The number of tasks that can run in parallel is limited to the number of cores. In this case, only two tasks can run at the same time because you’ve specified two cores for the executor.
If you have a Spark cluster where each node has multiple cores (let’s say 6 cores for this example), and you start two applications where:

- The first application is configured to use 2 cores
- The second application is configured to use 4 cores

Then, each node will use 2 cores for the first application and 4 cores for the second application, utilizing all 6 cores on the node.

This means that both applications will run in parallel on the same node, with the first application running tasks on 2 cores and the second application running tasks on 4 cores.

However, keep in mind that this is a simplified explanation. In a real-world scenario, resource allocation can be more complex and depends on the specific configurations of your Spark applications and cluster, as well as the cluster manager you’re using (like YARN, Mesos, or Spark’s standalone cluster manager).

The common areas or reasons for Apache Spark application failure on a cluster are typically related to:

- **User Code**: Errors in the user’s code can cause an application to fail. This could be due to logic errors, incorrect use of Spark APIs, unhandled exceptions, etc.
- **Configuration**: Incorrect or suboptimal configuration of Spark properties can lead to failures. This includes settings related to memory allocation, number of cores, network timeouts, etc.
- **Application Dependencies**: Missing or incompatible libraries required by the application can cause failures.
- **Resource Allocation**: Insufficient resources (like CPU, memory, or disk space) on the cluster nodes can lead to application failures. This could be due to over-allocation of resources to Spark executors, leaving insufficient resources for Spark’s internal operations.
- **Network Communication**: Issues in network communication between the nodes of the Spark cluster can lead to failures. This could be due to network failures, firewall settings, etc.

| Package/Method         | Description                                                                                                                                                                                                                                                                                                                                                                                                                                               | Code Example                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |     |
| ---------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --- |
| agg()                  | Used to get the aggregate values like count, sum, avg, min, and max for each group.                                                                                                                                                                                                                                                                                                                                                                       | 1. 1<br><br>1. agg_df = df.groupBy("column_name").agg({"column_to_aggregate": "sum"}) <br><br>Copied!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |     |
| cache()                | Apache Spark transformation that is often used on a DataFrame, data set, or RDD when you want to perform more than one action. cache() caches the specified DataFrame, data set, or RDD in the memory of your cluster's workers. Since cache() is a transformation, the caching operation takes place only when a Spark action (for example, count(), show(), take(), or write()) is also used on the same DataFrame, Dataset, or RDD in a single action. | 1. 1<br>2. 2<br><br>1. df = spark.read.csv("customer.csv")<br>2. df.cache()  <br><br>Copied!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |     |
| cd                     | Used to move efficiently from the existing working directory to different directories on your system.                                                                                                                                                                                                                                                                                                                                                     | Basic syntax of the cd command:<br><br>1. 1<br><br>1. cd [options]… [directory] <br><br>Copied!<br><br>Example 1: Change directory location to folder1.<br><br>1. 1<br><br>1. cd /usr/local/folder1 <br><br>Copied!<br><br>Example 2: Get back to the previous working directory.<br><br>1. 1<br><br>1. cd - <br><br>Copied!<br><br>Example 3: Move up one level from the present working directory tree.<br><br>1. 1<br><br>1. cd .. <br><br>Copied!                                                                                                                                                                                                                                                                                                                                                                                                                                      |     |
| def                    | Used to define a function. It is placed before a function name that is provided by the user to create a user-defined function.                                                                                                                                                                                                                                                                                                                            | 1. 1<br><br>1. def greet(name): <br><br>Copied!<br><br>This function takes a name as a parameter and prints a greeting.<br><br>1. 1<br><br>1. print(f"Hello, {name}!")<br><br>Copied!<br><br>Calling the function:<br><br>1. 1<br><br>1. greet("John")<br><br>Copied!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |     |
| docker exec            | Runs a new command in a running container. Only runs while the container's primary process is running, and it is not restarted if the container is restarted.                                                                                                                                                                                                                                                                                             | 1. 1<br>2. 2<br><br>1. docker exec -it container_name command_to_run<br>2. docker exec -it my_container /bin/bash <br><br>Copied!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |     |
| docker rm              | Used to remove one or more containers.                                                                                                                                                                                                                                                                                                                                                                                                                    | To remove a single container by name or ID:<br><br>1. 1<br><br>1. docker rm container_name_or_id <br><br>Copied!<br><br>To remove multiple containers by specifying their names or IDs:<br><br>1. 1<br><br>1. docker rm container1_name_or_id container2_name_or_id <br><br>Copied!<br><br>To remove all stopped containers:<br><br>1. 1<br><br>1. docker rm $(docker ps -aq) <br><br>Copied!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |     |
| docker run             | It runs a command in a new container, getting the image and starting the container if needed.                                                                                                                                                                                                                                                                                                                                                             | 1. 1<br><br>1. docker run [OPTIONS] IMAGE [COMMAND] [ARG...] <br><br>Copied!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |     |
| for                    | The _for_ loop operates on lists of items. It repeats a set of commands for every item in a list.                                                                                                                                                                                                                                                                                                                                                         | 1. 1<br><br>1. fruits = ["apple", "banana", "cherry"] <br><br>Copied!<br><br>Iterating through the list using a _for_ loop for fruit in fruits:<br><br>1. 1<br><br>1. print(f"I like {fruit}s")<br><br>Copied!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |     |
| groupby()              | Used to collect the identical data into groups on DataFrame and perform count, sum, avg, min, max functions on the grouped data.                                                                                                                                                                                                                                                                                                                          | 1. 1<br><br>1. import pandas as pd <br><br>Copied!<br><br>Sample DataFrame:<br><br>1. 1<br>2. 2<br>3. 3<br><br>1. data = {'Category': ['A', 'B', 'A', 'B', 'A', 'B'], <br>2.         'Value': [10, 20, 15, 25, 30, 35]}<br>3. df = pd.DataFrame(data) <br><br>Copied!<br><br>Grouping by "Category" and performing aggregation operations:<br><br>1. 1<br>2. 2<br><br>1. grouped = df.groupby('Category').agg({'Value': ['count', 'sum', 'mean', 'min', 'max']}) <br>2. print(grouped) <br><br>Copied!                                                                                                                                                                                                                                                                                                                                                                                     |     |
| repartition()          | Used to increase or decrease the RDD or DataFrame partitions by number of partitions or by a single column name or multiple column names.                                                                                                                                                                                                                                                                                                                 | Create a sample DataFrame:<br><br>1. 1<br>2. 2<br>3. 3<br><br>1. data = [("John", 25), ("Peter", 30), ("Julie", 35), ("David", 40), ("Eva", 45)] <br>2. columns = ["Name", "Age"] <br>3. df = spark.createDataFrame(data, columns) <br><br>Copied!<br><br>Show the current number of partitions.<br><br>1. 1<br><br>1. print("Number of partitions before repartitioning: ", df.rdd.getNumPartitions()) <br><br>Copied!<br><br>Repartition the DataFrame to 2 partitions.<br><br>1. 1<br><br>1. df_repartitioned = df.repartition(2)<br><br>Copied!<br><br>Show the number of partitions after repartitioning.<br><br>1. 1<br><br>1. print("Number of partitions after repartitioning: ", df_repartitioned.rdd.getNumPartitions())<br><br>Copied!<br><br>Stop the SparkSession.<br><br>1. 1<br><br>1. spark.stop() <br><br>Copied!                                                         |     |
| return                 | Used to end the execution of the function call and returns the result (value of the expression following the return keyword) to the caller.                                                                                                                                                                                                                                                                                                               | 1. 1<br>2. 2<br>3. 3<br><br>1. def add_numbers(a, b): <br>2.     result = a + b <br>3.     return result <br><br>Copied!<br><br>Calling the function and capturing the returned value:<br><br>1. 1<br><br>1. sum_result = add_numbers(5, 6) <br><br>Copied!<br><br>Printing the result.<br><br>1. 1<br><br>1. print("The sum is:", sum_result)<br><br>Copied!<br><br>Output.<br><br>1. 1<br><br>1. The sum is: 11<br><br>Copied!                                                                                                                                                                                                                                                                                                                                                                                                                                                           |     |
| show()                 | Spark DataFrame show() is used to display the contents of the DataFrame in a table row and column format. By default, it shows only 20 rows, and the column values are truncated at 20 characters.                                                                                                                                                                                                                                                        | 1. 1<br><br>1. df.show() <br><br>Copied!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |     |
| spark.read.csv(“path”) | Using this, you can read a CSV file with fields delimited by pipe, comma, tab (and many more) into a Spark DataFrame.                                                                                                                                                                                                                                                                                                                                     | 1. 1<br><br>1. from pyspark.sql import SparkSession <br><br>Copied!<br><br>Create a SparkSession.<br><br>1. 1<br><br>1. spark = SparkSession.builder.appName("CSVReadExample").getOrCreate() <br><br>Copied!<br><br>Read a CSV file into a Spark DataFrame.<br><br>1. 1<br><br>1. df = spark.read.csv("path_to_csv_file.csv", header=True, inferSchema=True) <br><br>Copied!<br><br>Show the first few rows of the DataFrame.<br><br>1. 1<br><br>1. df.show() <br><br>Copied!<br><br>Stop the SparkSession.<br><br>1. 1<br><br>1. spark.stop() <br><br>Copied!                                                                                                                                                                                                                                                                                                                             |     |
| wget                   | Stands for web get. The wget is a free noninteractive file downloader command. Noninteractive means that it can work in the background when the user is not logged in.                                                                                                                                                                                                                                                                                    | Basic syntax of the wget command; commonly used options are `[-V], [-h], [-b], [-e], [-o], [-a], [-q]`<br><br>1. 1<br><br>1. wget [options]… [URL]… <br><br>Copied!<br><br>Example 1: Specifies to download file.txt over HTTP website URL into the working directory.<br><br>1. 1<br><br>1. wget http://example.com/file.txt <br><br>Copied!<br><br>Example 2: Specifies to download the archive.zip over HTTP website URL in the background and returns you to the command prompt in the interim.<br><br>1. 1<br><br>1. wget -b http://www.example.org/files/archive.zip <br><br>Copied!                                                                                                                                                                                                                                                                                                 |     |
| withColumn()           | Transformation function of DataFrame which is used to change the value, convert the datatype of an existing column, create a new column, and many more.                                                                                                                                                                                                                                                                                                   | Sample DataFrame:<br><br>1. 1<br>2. 2<br>3. 3<br><br>1. data = [("John", 25), ("Peter", 30), ("David", 35)]<br>2. columns = ["Name", "Age"]<br>3. df = spark.createDataFrame(data, columns) <br><br>Copied!<br><br>Using withColumn to create a new column and change values<br><br>1. 1<br>2. 2<br>3. 3<br>4. 4<br>5. 5<br>6. 6<br>7. 7<br><br>1. updated_df = df \ <br>2.     .withColumn("DoubleAge", col("Age") * 2)  # Create a new column "DoubleAge" by doubling the "Age" column <br>3. updated_df = updated_df \ <br>4.     .withColumn("AgeGroup", when(col("Age") <= 30, "Young") <br>5.                 .when((col("Age") > 30) & (col("Age") <= 40), "Middle-aged") <br>6.                 .otherwise("Old"))  # Create a new column "AgeGroup" based on conditions <br>7. updated_df.show() <br><br>Copied!<br><br>Stop the SparkSession.<br><br>1. 1<br><br>1. spark.stop() |     |
|                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |     |
|                        |                                                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |     |

## Monitoring and Tuning 
| erm                               | Definition                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Apache Spark user interface**   | Provides valuable insights, organized on multiple tabs, about the running application. The **Jobs** tab displays all jobs in the application, including job status. The **Stages** tab reports the state of tasks within a stage. The **Storage** tab shows the size of any RDDs or DataFrames that persisted to memory or disk. The **Environment** tab includes any environment variables and system properties for Spark or the JVM. The **Executor** tab displays a summary that shows memory and disk usage for any executors in use for the application. Additional tabs display based on the type of application in use.                                                                                                                                                                                                                                           |
| **Application dependency issues** | Spark applications can have many dependencies, including application files such as Python script files, Java JAR files, and even required data files. Applications depend on the libraries used and their dependencies. Dependencies must be made available on all cluster nodes, either by preinstallation, including the dependencies in the Spark-submit script bundled with the application, or as additional arguments.                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| **Application resource issues**   | CPU cores and memory can become an issue if a task is in the scheduling queue and the available workers do not have enough resources to run the tasks. As a worker finishes a task, the CPU and memory are freed, allowing the scheduling of another task. However, if the application asks for more resources that can ever become available, the tasks might never be run and eventually time out. Similarly, suppose that the executors are running long tasks that never finish. In that case, their resources never become available, which also causes future tasks to never run, resulting in a timeout error. Users can readily access these errors when they view the UI or event logs.                                                                                                                                                                          |
| **Application-id**                | A unique ID that Spark assigns to each application. These log files appear for each executor and driver process that the application runs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| **Data validation**               | The practice of verifying the integrity, quality, and correctness of data used within Spark applications or data processing workflows. This validation process includes checking data for issues such as missing values, outliers, or data format errors. Data validation is crucial for ensuring that the data being processed in Spark applications is reliable and suitable for analysis or further processing. Various techniques and libraries, such as Apache Spark's DataFrame API or external tools, can be employed to perform data validation tasks in Spark environments.                                                                                                                                                                                                                                                                                      |
| **Directed acyclic graph (DAG)**  | Conceptual representation of a series of activities. A graph depicts the order of activities. It is visually presented as a set of circles, each representing an activity, some connected by lines, representing the flow from one activity to another.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| **Driver memory**                 | Refers to the memory allocation designated for the driver program of a Spark application. The driver program serves as the central coordinator of tasks, managing the distribution and execution of Spark jobs across cluster nodes. It holds the application's control flow, metadata, and the results of Spark transformations and actions. The driver memory's capacity is a critical factor that impacts the feasibility and performance of Spark applications. It should be configured carefully to ensure efficient job execution without memory-related issues.                                                                                                                                                                                                                                                                                                    |
| **Environment tab**               | Entails several lists to describe the environment of the running application. These lists include the Spark configuration properties, resource profiles, properties for Hadoop, and the current system properties.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| **Executor memory**               | Used for processing. If caching is enabled, additional memory is used. Excessive caching results in out-of-memory errors.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| **Executors tab**                 | A component of certain distributed computing frameworks and tools used to manage and monitor the execution of tasks within a cluster. It typically presents a summary table at the top that displays relevant metrics for active or terminated executors. These metrics may include task-related statistics, data input and output, disk utilization, and memory usage. Below the summary table, the tab lists all the individual executors that have participated in the application or job, which may include the primary driver. This list often provides links to access the standard output (stdout) and standard error (stderr) log messages associated with each executor process. The Executors tab serves as a valuable resource for administrators and operators to gain insights into the performance and behavior of cluster executors during task execution. |
| **Job details**                   | Provides information about the different stages of a specific job. The timeline displays each stage, where the user can quickly see the job's timing and duration. Below the timeline, completed stages are displayed. In the parentheses beside the heading, users will see a quick view that displays the number of completed stages. Then, view the list of stages within the job and job metrics, including when the job was submitted, input or output sizes, the number of attempted tasks, the number of succeeded tasks, and how much data was read or written because of a shuffle.                                                                                                                                                                                                                                                                              |
| **Jobs tab**                      | Commonly found in Spark user interfaces and monitoring tools, it offers an event timeline that provides key insights into the execution flow of Spark applications. This timeline includes crucial timestamps such as the initiation times of driver and executor processes, along with the creation timestamps of individual jobs within the application. The Jobs tab serves as a valuable resource for monitoring the chronological sequence of events during Spark job execution.                                                                                                                                                                                                                                                                                                                                                                                     |
| **Multiple related jobs**         | Spark application can consist of many parallel and often related jobs, including multiple jobs resulting from multiple data sources, multiple DataFrames, and the actions applied to the DataFrames.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| **Parallelization**               | Parallel regions of program code executed by multiple threads, possibly running on multiple processors. Environment variables determine the number of threads created and calls to library functions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| **Parquet**                       | A columnar format that is supported by multiple data processing systems. Spark SQL allows reading and writing data from Parquet files, and Spark SQL preserves the data schema.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| **Serialization**                 | Required to coordinate access to resources that are used by more than one program. An example of why resource serialization is needed occurs when one program is reading from a data set and another program needs to write to the data set.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| **Spark data persistence**        | Also known as caching data in Spark. Ability to store intermediate calculations for reuse. This is achieved by setting persistence in either memory or both memory and disk. Once intermediate data is computed to generate a fresh DataFrame and cached in memory, subsequent operations on the DataFrame can utilize the cached data instead of reloading it from the source and redoing previous computations. This feature is crucial for accelerating machine learning tasks that involve multiple iterations on the same data set during model training.                                                                                                                                                                                                                                                                                                            |
| **Spark History server**          | Web UI where the status of running and completed Spark jobs on a provisioned instance of Analytics Engine powered by Apache Spark is displayed. If users want to analyze how different stages of the Spark job are performed, they can view the details in the Spark history server UI.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| **Spark memory management**       | Spark memory stores the intermediate state while executing tasks such as joining or storing broadcast variables. All the cached and persisted data will be stored in this segment, specifically in the storage memory.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| **Spark RDD persistence**         | Optimization technique that saves the result of RDD evaluation in cache memory. Using this technique, the intermediate result can be saved for future use. It reduces the computation overhead.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| **Spark standalone**              | Here, the resource allocation is typically based on the number of available CPU cores. By default, a Spark application can occupy all the cores within the cluster, which might lead to resource contention if multiple applications run simultaneously. In the context of the standalone cluster manager, a ZooKeeper quorum is employed to facilitate master recovery through standby master nodes. This redundancy ensures high availability and fault tolerance in the cluster management layer. Additionally, manual recovery of the master node can be achieved using file system operations in the event of master failure, allowing system administrators to intervene and restore cluster stability when necessary.                                                                                                                                              |
| **SparkContext**                  | When a Spark application is being run, as the driver program creates a SparkContext, Spark starts a web server that serves as the application user interface. Users can connect to the UI web server by entering the hostname of the driver followed by port 4040 in a browser once that application is running. The web server runs for the duration of the Spark application, so once the SparkContext stops, the server shuts down, and the application UI is no longer accessible.                                                                                                                                                                                                                                                                                                                                                                                    |
| **Stages tab**                    | Displays a list of all stages in the application, grouped by the current state of either completed, active, or pending. This example displays three completed stages. Click the Stage ID Description hyperlinks to view task details for that stage.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| **Storage tab**                   | Displays details about RDDs that have been cached or persisted to memory and written to disk.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| **Unified memory**                | Unified regions in Spark shared by executor memory and storage memory. If executor memory is not used, storage can acquire all the available memory and vice versa. If the total storage memory usage falls under a certain threshold, executor memory can discard storage memory. Due to complexities in implementation, storage cannot evict executor memory.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| **User code**                     | Made up of the driver program, which runs in the driver process, and the functions and variables serialized that the executor runs in parallel. The driver and executor processes run the application user code of an application passed to the Spark-submit script. The user code in the driver creates the SparkContext and creates jobs based on operations for the DataFrames. These DataFrame operations become serialized closures sent throughout the cluster and run on executor processes as tasks. The serialized closures contain the necessary functions, classes, and variables to run each task.                                                                                                                                                                                                                                                            |
| **Workflows**                     | Include jobs created by SparkContext in the driver program. Jobs in progress run as tasks in the executors, and completed jobs transfer results back to the driver or write to disk.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |

, the data is divided into partitions, each of which is a small chunk of the total data. Each partition can be processed independently on a different node in the Spark cluster, allowing for parallel processing and thus improving the performance of Spark computations.

- The Spark application UI centralizes critical information, including status information, into the Jobs, Stages, Storage, Environment, and Executors tabbed regions.
    
- You can quickly identify failures and then drill down to the lowest levels of the application to discover their root causes. If the application runs SQL queries, select the SQL tab and the description hyperlink to display the query’s details.
    
- The Spark application workflow includes jobs created by the Spark Context in the driver program, jobs in progress running as tasks in the executors, and completed jobs transferring results back to the driver or writing to disk.
    
- Common reasons for application failure on a cluster include user code, system and application configurations, missing dependencies, improper resource allocation, and network communications.
    
- User code-specific errors include syntax, serialization, and data validation. Related errors can happen outside the code. If a task fails due to an error, Spark can attempt to rerun tasks for a set number of retries.
    
- If all attempts to run a task fail, Spark reports an error to the driver and terminates the application. The cause of an application failure can usually be found in the driver event log. 
    
- Spark enables configurable memory for executor and driver processes. Executor memory and storage memory share a region that can be tuned.
    
- Setting data persistence by caching data is one technique used to improve application performance.
    
- Spark assigns processor cores to driver and executor processes during application processing. Executors process tasks in parallel according to the number of cores available or as assigned by the application. 
    
- You can apply the argument ‘--executor-cores 8 \’ to set executor cores on submit _per executor_. This example specifies eight cores.
    
- You can specify the executor cores for a Spark standalone cluster _for the application_ using the argument ‘--total-executor-cores 50’ followed by the number of cores for the application. This example specifies 50 cores.
    
- When starting a worker manually in a Spark standalone cluster, you can specify the number of cores the application uses by using the argument ‘--cores’ followed by the number of cores. Spark’s default behavior is to use all available cores.
