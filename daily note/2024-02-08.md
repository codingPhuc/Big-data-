# how to access databases using Python 
- Explain how to used Python to connect to databases 
- Create tables , load data , and query data using SQL form jupyter notebook 
- Create an instance in the cloud and connect to a database 
- describe SQL API  
- List API that are used for popular SQL based DBMS systems 
## Benefit of pythong 
- python ecosystem : Numpy , pandas , matplotlib , Scipy 
- ease of use 
- portable 
- python support relational database system  
- python detailed documentation 
## introduction to note books 
notebook alllows creating and sharing documents containing live codes equation , visualization , and explantory text 
### what are jupyter notebooks 
- language of choice : python R 
- share notebook   
- interactive output : html images , latex , 
- big data intgegration 
![[image.PNG]]
## application program interface

![[database.PNG]]

# writing code using DB-API 
the user write python program using jupyter notebook 
- python standard API  for accessing relational databases  
- allow a single program that to work with multiple kinds of relational databases 
## benefit of using db API 
easy to implement and understand 
encourages similarity between the python modules used to access databases 
achieves consistency 
portable across databases 
broad reach of database connectivity from python 
![[differenet connection databases.PNG]]
## Concept of the Python DB API 
### Connection Objects 

1. **Connection Objects**: These are created using the connect() function of the DB API. Once you have a connection, you can create a Cursor object. Connection objects are responsible for managing transactions. Here’s an example:

```python
import sqlite3
connection = sqlite3.connect("my_database.db")
```

2. **Managing Transactions**: In the context of a relational database, a transaction is a sequence of operations performed as a single logical unit of work. A transaction has ACID properties - Atomicity, Consistency, Isolation, and Durability. Connection objects are responsible for managing transactions, which can be initiated or ended using the `commit()` or `rollback()` methods.

```python
try:
    # execute some queries
    connection.commit()
except:
    connection.rollback()
```

The **Connection object** is used to establish a connection with the database. It’s like a bridge between your Python program and the database.
### Cursor Object 
Sure, I’d be happy to explain these concepts in more detail. The Python DB API defines a database-neutral interface to data stored in relational databases. Here are the key concepts:

3. **Cursor Object**: Cursor objects allow Python code to execute PostgreSQL command in a database session. Cursors created from the same connection are not isolated, i.e., any changes done to the database by a cursor are immediately visible by the other cursors.

```python
cursor = connection.cursor()
```

4. **Database Queries**: You can execute SQL commands in the database via the `execute()` method of a cursor object.

```python
cursor.execute("SELECT * FROM my_table")
```

5. **Scrolling Through Result Set**: You can retrieve a single row using the `fetchone()` method, or multiple rows using the `fetchmany(size)` method, or all rows using the `fetchall()` method.

```python
rows = cursor.fetchall()
for row in rows:
    print(row)
```

6. **Retrieving Results**: After executing a query, you can retrieve the result set by calling `fetchone()`, `fetchmany()`, or `fetchall()`.

```python
rows = cursor.fetchall()
for row in rows:
    print(row)
```

Remember to always close the connection once you’re done with your database operations. This can be done using the `close()` method of the connection object.

```python
connection.close()
```

The **Cursor object**, on the other hand, is used to execute SQL commands and fetch data from the database. It’s like a messenger that communicates between your Python program and the database through the connection.
## what are connections methods 
.cursor() 
.commit()
.rollback() 
.close()


# ACCESSING DATABASE USING SQL MAGIC  
- Describe magic statements 
- Differentiate line magics and cell magics 
- discuss some popularly used magic statement 
- explain the use of SQL magic in Jupyter notebooks to access databases 
Magic  command are special commands that provide special functionalities 
- the are not valid python code but affect the behavior of the notebook 
- they are designed to solve standard data analysis problems 
## type of cell magics 
### line magics 
commands that are prefixed with a single % character and operate on a single line of input 
### cell magic  
commad that are prefixed with two `%%` character and operate on multiple lines input 

![[line magic.PNG]]
- these are popular line magic  
- line magic can also be used as a cell magic statement 

sumary  : 
- Magic commands are special commands that provide special functionalities.
    
- Cell magics are commands prefixed with two %% characters and operate on multiple input lines. 
    
- DB APIs are commands prefixed with two %% characters and operate on multiple input lines.
    
- The two main concepts in the Python DB API are Connection Objects and Query Objects.
    
- A database cursor is a control structure that enables traversal over the records in a database. 
    
- Pandas’ methods are equipped with common mathematical and statistical methods.  
    
- The pandas.read_csv functionis used to read the database CSV file.
    
- The SQLite3.connect functionis used to connect to a database.
    
- To use pandas to retrieve data from the database tables, load data using the read_sql method and select the SQL Select Query
    
- A categorical scatterplot is created using the swarmplot() method by the seaborn package.





# Big Data 
## who can benefit from this course 
- data engineer and architects 
- data scientist and ML engineers 
- technology adoptiion influencers and mangers 
# Module 1 : What is Big Data ? 
![[what you will learn in big data.PNG]]
## Module 2 : introduction to the Dadoop ecosystem 
![[hadoop architecture.PNG]]
## Module 3 : Apache Spark 
- Functional programming 
- Lambda functions 
- Resilient Distributed Datasets 
- Parallel programming 
- Resilience 
- Queries 
- Function , part and benefits of Spark SQL and dataFrame queries 
## Module 4 : Data Frames and Spark SQL 
![[module_4data fram and spark.PNG]]
## Module 5 : Development and Runtime Environment Options 
![[cloud computing.PNG]]
## Module 6 : Monitoring and Tuning 
![[apache.PNG]]
## module 7 : final Project and Assessment 
![[module7.PNG]]
## note 
learn by doing do the hand on lab in the learning module 
watch all video and comple all quiz 
used the discussion form to collaborate with peers 
## what is Big Data 
after watching the video 
- explain big data 
- identify the characteristed of big data 
- explain the five V of big data 
Bernard Marr 
## big Data vs Small data 
Small Data 
- Small enough for human inference 
- accumulated slowly 
- relatively consistent and structured data usually stored in known forms such as JSON and XML 
- mostly located in storage system within enterprises or data centers 
Big Data  
- Data generated in huge volumes  and could be structured , semi-structured  , or unstructured 
- Need processing to generate insights for human consumption 
- Arrives continuously at enormous speed from multiple sources 
- comprises any form of data including video , photos , and more 
- Distributed on the cloud and server farms 
## Big Data Life Cycle 
Big data is the entired life cycle of working with large volumes of data 

![[big data circle.PNG]]
Data Collection 
- initiated as the result of business problem 
Data Collection
- as data is collected , it get stored suing a framework for distributed stored 
Data Modeling 
- to make sense of all the data collected , map and reduce task and script created a data model to stored in a a database , this model is the relationship between the data 
Data Processing 
- after modeling data is ready to be process tools such as APache Spark are used to produce meaningful information form the modeled data 
Data Visualization :
- data is presented in a graphical format 
- this will lead to a meaning full business cases 
- continuing the life cycle 



## The Four V of Big Data
## Velocity 
- Velocity is the speed at which the data is arrive 
Description 
-  Data that is generated  fast
- Process that never stops
 Attributes 
- Batch 
- Close to real time 
- Streaming 
Drivers 
- Improved connectivity and hardware
- Rapid response times 
- Precalculated analysis 
### Volume 
Description 
- Scale of data 
- Increase amount of stored data 
 Attributes 
- Petabytes 
- Exa 
- Zetta 
 Drivers 
- Increase in data sources 
- Higher resolution sensors 
## Variety 
- Data that come from machines , people , processess 
- Structured , semi- structured , and unstructured data 
Attributes 
- Structure , complexity , and origin 
Drivers 
- Mobile technologies 
- Scalable infrastructure 
- resilience 
- fault recovery 


### Veracity 
Description  
- Quality , origin , and conformity of fats 
- Accuracy of data 
- data that comes form people and processes 
Attribute 
- consistency and completeness 
- integrity 
- ambiguity 
Driver 
- cost and traceability 
- robust ingestion 
- ETL mechanisms 


Adding the fifth V of Big Data 
Creating the most amount of money and value 
Big data is the digital trace  that get generated throught the entire digital ecosystem 
Big data is a hige volumn
# Impact of Big data 
- list example of big data related technologies 
- explain the impact of big data 

## Generating and using Big Data 
- photo video and text people send to each other 
Big data in daily life 
recommendation engines use data from 
- product searches 
- past orders 
- item in shopping carts 
- customer rating and likes 
- what other shoppers have looked at and bought 
Big data impacte people 
virtual personal assistant 
- use Big data to devise answers 
- use advanced neural network 
- process speech into text 
- trnslate text into task 
- they include external factor 
- google now make recommendation bfore the user asks for them 
	- they will tell the user weather and road trafic before the even need it 
- Big data is chaging the way buiness is operating 
- data scientist and big data engineers  
## internet of thing 
an internet enabled connected  network of smart devices such as sensor , processors , embedded devices and communication hardware 

Data collected , analyzed , and acted upon for benefit such as improving customer experience , enhance productivity , and increased revenue 
![[major component of big data.PNG]]
Thing of device 
- an internet enable  connected  network of system  that comprise smart devices and sensors 
- product and transmit a variety of data 

Gateway
- route the data to somewhere in the cloud 
Cloud  
- store the data 
Analytic 
- data engineer will then process the data using an algorithm  to make sense of this collected data then comprise into an analysist form  
User Interface 
- the analyzed data is then used to make business and consumer decision to enhance the customer experience 

summary 
- Big data is everywhere , and is being collected and used to drive business decision and influence people lives 
- personal assistant like Siri or Alexa use big data to devise answers 
- google now uses big data to 
- Google now uses Big Data to forecast future need and behavior 
- Internet of Thing or IoT devices continually generate massive volumes of Data 
- Big data analytics helps companies gain insights from the data collect by Iot devices 
# Parallel Processing and Scalability 
objective 
after watching this video 
Explain why Big data requres parralel processing 
identify the difference between linear and parallel processing 
# why Parallel processing ? 
![[single node processing.PNG]]
- the functionality of the computer is to move data form compute capacity to the storage area 

## liner vs parallel prcessing 
![[linear processing.PNG]]
- linear processing is the method in which the  problem is slove linearly 
- the problem is slove in each step if there occur an error then problem will be reverted back to it initial  state 

### parallel processing 
### parallel processing 

![[parrele processing.PNG]]
- the problem is split into different instruction , because the problem is being run on different execution node 
- error can be fix and execute locally  
why parallel processing is apt for big data 
Parallel processing advantages 
- Parallel processing approach can process large dataset in a fraction of time 
- less memory and compute requirement needed as set of instruction are distributed to smaller execution nodes 
- More execution nodes can be added or removed from the processing network depending on complexity of the problem 
## Data Scaling in Big data 
![[scaling up.PNG]] 
when your data  increase beyound the amount of data that your node can handle it the node need to increase it capacity , this is call vertical scaling 
### horizontal scaling 

![[horizontal scaling.PNG]]
A better strategy , or at least the one most people ultimately choose , is to scale out  horizontally 
this mean adding node with the same capacity which is call computing cluster 
computing cluster can solve " embarrassingly parallel"  these are the kind of workload that can easily be divided and run independent of one another 
if any one process fail it have no impact on the others and can simply be return 
"not so easy parallel " 
![[not so easy.PNG]]
these kind of workload requires sending messages across a network to each other or writing them to a file system  that is accessible to all process in the cluster 
the complexity is increase you are asking multiple computer to act as a single computer   
![[cluster.PNG]]
the ideal of Hadoop of bringing the ocmputation to the data is that instead of spliting the data into different part speard in different machine , the computation will take place in the main node and outputed in the main node 

## fault tolerance  
fault tolerance meaning that the data will still be process even if a node fail 
if the first node ever go down you can add in another node into the cluster
![[hadoop processing.PNG]]

the Hadoop is best 99.9999% 
work for hdfs and f3 and object storage 
# Big data tool and ecosystem 
- identify the key tooling categories whitin the ibg data ecosystem 
- Describe the rile of each tool 
![[cateogry.PNG]]
there are six different category to big data tools 
data technology allow enterprises to : 
capture , process and share data tat any scale and in any format 
work with structured and unstructured data 
leverage higher  performance , parallel big data processing 
key technology include Hadoop , HDFS , Spark ,MapReduce 
## Analytics an Visualization 
Big data analytics examnies large amount of data 
analyzed data is visualized using graph , chart and maps 

Popular analytics tools available are Tableau , Palantir , SAS , Pentaho , and Teradata 
## Business Intelligence 
Business intelligence (BI) offers a range of tools that provide quick and easy way to transform data into actionable insights 
- such insight inform an organization strategic and tactical business decisions 
example include cognos , Oracle , PowerBi , Business Objects m and Hyperion 

## Cloud providers 
Offer fundamental infrastructure and support with shared resource including computing 
storage , networking , and analytical software 
## NoSQL
NoSql dataabse are best suited for big data processing 
- store and process vast amount of data at scale 
- Store information in Json document instead of relational table 
- NoSQL databse types include pure document databases , key-value store , wide_column database ,and graph databases 
Example inlcude MongoDb , CouchDB , Cassandra , Redis 
## Programing tools 
Big data programming tools : 
- perform large-scale analytical tasks  and operationalize Big Data 
- Provide all necessary function for the Big Data life cycle 
R , Python , SQL , Scaka , and Julia are common programming tools 
summary 
Big Data tools have six main tooling categories : 
- Data technologies 
- Analytics and visualuzation 
- Business intelligence 
- Cloud providers 
- NoSQL databases , and 
- Programming tools 




# Open source and Big Data 
- free 
- Source code is open the code can be review to use or re-user as needed in other project 
- also allow any user to propose changes to the project 


Open source linces type 
- public domain 
- copy left 
- permissive 
- Lesser General public license 
Why is the open source model used for big Data ? 
- large and complex project 
- project serve the need of many organizaiton 
- linux kernal development there was many os standard many year latter linux was the oj , this happend because the project took on the life of it own beyond any  company 


## The community garden of code

![[commiter processing.PNG]]
- the committer is the one responsible to modify the  code directly 
- The contributor are the person that submit their  version of the code , the code will only update if it is ok by the committer 
- the user is the user of the code 


Open source platforms 
the code is freely distributed 
Hadoop play a major role in open source Big Data projects : 
- hadoop MapReduce 
- Hadoop File System (HDFS)
- yet Another resource negotiator (Yarn )

